# 文本处理与解析

<cite>
**本文引用的文件**
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts)
- [lib/processing/processor.ts](file://lib/processing/processor.ts)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts)
- [lib/processing/index.ts](file://lib/processing/index.ts)
- [app/api/sources/upload/route.ts](file://app/api/sources/upload/route.ts)
- [app/api/sources/url/route.ts](file://app/api/sources/url/route.ts)
- [app/api/sources/text/route.ts](file://app/api/sources/text/route.ts)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts)
- [lib/config.ts](file://lib/config.ts)
- [types/pdf-parse.d.ts](file://types/pdf-parse.d.ts)
- [lib/utils/logger.ts](file://lib/utils/logger.ts)
- [scripts/test-adaptive-splitting.ts](file://scripts/test-adaptive-splitting.ts)
- [package.json](file://package.json)
</cite>

## 更新摘要
**变更内容**
- 重构Web解析器实现，实现三层处理策略：本地抓取+Readability解析+Jina Reader回退机制
- 增强反爬虫处理能力，包括User-Agent设置、超时控制和错误处理
- 实现内容质量评估机制，基于字数统计和提取质量的智能回退
- 新增@mozilla/readability和jsdom依赖，提升网页内容提取准确性
- 完善PDF检测和处理流程，支持Content-Type为PDF的URL自动转换

## 目录
1. [简介](#简介)
2. [项目结构](#项目结构)
3. [核心组件](#核心组件)
4. [架构总览](#架构总览)
5. [详细组件分析](#详细组件分析)
6. [依赖关系分析](#依赖关系分析)
7. [性能考量](#性能考量)
8. [故障排查指南](#故障排查指南)
9. [结论](#结论)
10. [附录](#附录)

## 简介
本文件面向"文本处理与解析"能力，系统化阐述以下主题：
- 文本切分算法：语义分割、句子边界检测、段落识别与自适应策略
- 多格式文本提取：PDF 文本提取、网页内容清理与纯文本处理
- 文本预处理：去重、标准化与噪声过滤
- 分块策略与重叠处理：平衡块大小与上下文完整性
- 质量评估与性能优化：token 估算、吞吐与内存控制
- 自定义规则与回退机制：特殊格式支持与失败兜底
- 扩展性与基准测试建议

## 项目结构
围绕"文本处理与解析"的核心目录与文件如下：
- 处理流程与切分：lib/processing/processor.ts、lib/processing/text-splitter.ts、lib/processing/content-analyzer.ts
- 格式提取：lib/processing/pdf-parser.ts、lib/processing/web-parser.ts
- 向量化与持久化：lib/processing/embedding.ts、lib/db/vector-store.ts
- API 入口：app/api/sources/upload/route.ts、app/api/sources/url/route.ts、app/api/sources/text/route.ts
- 配置与类型：lib/config.ts、types/pdf-parse.d.ts、lib/utils/logger.ts

```mermaid
graph TB
subgraph "API 层"
U["上传 PDF<br/>sources/upload"]
V["添加 URL<br/>sources/url"]
T["添加文字<br/>sources/text"]
end
subgraph "处理层"
P["处理器<br/>processor.ts"]
S["文本切分器<br/>text-splitter.ts"]
A["内容分析器<br/>content-analyzer.ts"]
E["嵌入生成<br/>embedding.ts"]
VS["向量存储<br/>vector-store.ts"]
end
subgraph "提取层"
PF["PDF 解析<br/>pdf-parser.ts"]
WP["网页解析<br/>web-parser.ts"]
end
U --> P
V --> P
T --> P
P --> PF
P --> WP
P --> S
S --> A
P --> E
E --> VS
```

**图表来源**
- [lib/processing/processor.ts](file://lib/processing/processor.ts#L1-L560)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L1-L432)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L1-L476)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L1-L150)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L1-L307)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L1-L189)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L1-L446)
- [app/api/sources/upload/route.ts](file://app/api/sources/upload/route.ts#L1-L111)
- [app/api/sources/url/route.ts](file://app/api/sources/url/route.ts#L1-L167)
- [app/api/sources/text/route.ts](file://app/api/sources/text/route.ts#L1-L122)

**章节来源**
- [lib/processing/index.ts](file://lib/processing/index.ts#L1-L51)

## 核心组件
- 文本切分器：基于递归字符切分，优先自然边界，支持自适应配置与重叠窗口
- **内容分析器**：**新增** 计算信息熵、符号密度、换行密度，识别内容类型，驱动自适应切分
- PDF 解析器：从 Supabase Storage 下载并解析 PDF，构建页码映射，检测扫描件
- **网页解析器**：**重构** 实现三层处理策略（本地抓取+Readability解析+Jina Reader回退机制），增强反爬虫处理能力
- 处理器：统一编排下载/抓取、解析、切分、向量化、写库的完整流程
- 嵌入生成：批量调用智谱 Embedding API，指数退避重试与去重
- 向量存储：批量插入、相似度检索、混合检索（向量+全文）

**章节来源**
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L1-L432)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L1-L476)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L1-L150)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L1-L307)
- [lib/processing/processor.ts](file://lib/processing/processor.ts#L1-L560)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L1-L189)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L1-L446)

## 架构总览
下图展示"从知识源到向量索引"的端到端流程，覆盖 PDF、URL、文字三种来源。

```mermaid
sequenceDiagram
participant Client as "客户端"
participant API as "API 路由"
participant Proc as "处理器 processor.ts"
participant PDF as "PDF 解析 pdf-parser.ts"
participant WEB as "网页解析 web-parser.ts"
participant Split as "切分 text-splitter.ts"
participant Analyzer as "分析 content-analyzer.ts"
participant Emb as "嵌入 embedding.ts"
participant VS as "向量存储 vector-store.ts"
Client->>API : "提交 PDF/URL/文字"
API->>Proc : "创建 Source 并入队"
Proc->>Proc : "状态更新 pending"
alt "PDF 来源"
Proc->>PDF : "下载并解析"
PDF-->>Proc : "文本/页码/统计"
else "URL 来源"
Proc->>WEB : "三层处理策略<br/>本地抓取 -> Readability -> Jina Reader"
WEB-->>Proc : "正文/标题/统计"
else "文字来源"
Proc->>Proc : "直接进入切分"
end
Proc->>Split : "切分文本(可自适应)"
Split->>Analyzer : "内容分析(信息熵/密度)"
Analyzer-->>Split : "自适应配置"
Split-->>Proc : "Chunk 列表"
Proc->>Emb : "批量生成向量(带重试)"
Emb-->>Proc : "带向量的 Chunk"
Proc->>VS : "批量写入向量库"
VS-->>Proc : "写入完成"
Proc-->>API : "状态更新 ready/failed"
API-->>Client : "返回结果"
```

**图表来源**
- [lib/processing/processor.ts](file://lib/processing/processor.ts#L80-L397)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L133-L149)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L226-L269)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L109-L189)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L140-L188)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L77-L173)

## 详细组件分析

### 三层网页处理策略与反爬虫机制
**重构** 网页解析器实现了全新的三层处理策略，显著提升了网页内容提取的稳定性和准确性：

#### 三层处理策略
- **本地抓取**：使用自定义User-Agent和超时控制，遵循重定向规则
- **Readability解析**：使用@mozilla/readability提取正文内容，支持JavaScript渲染页面
- **Jina Reader回退**：当本地解析失败或内容质量不足时，自动切换到Jina Reader服务

#### 反爬虫处理能力
- **User-Agent伪装**：使用真实浏览器User-Agent，模拟正常用户行为
- **超时控制**：全局超时30秒，读取内容也有独立超时保护
- **错误处理**：针对401、403、404等HTTP状态码提供明确的错误信息
- **内容质量评估**：基于字数统计和提取质量决定是否触发回退机制

#### 内容质量评估与智能回退
- **质量阈值**：当提取内容少于50个字或解析失败时触发回退
- **内容比较**：比较Jina Reader和本地解析的结果，选择更优质的内容
- **标题优先级**：优先保留本地解析的标题，如果Jina Reader没有标题则使用本地标题

```mermaid
flowchart TD
Start(["开始网页处理"]) --> LocalFetch["本地抓取<br/>30秒超时<br/>User-Agent伪装"]
LocalFetch --> CheckPDF{"Content-Type为PDF？"}
CheckPDF -- "是" --> PDFFlow["PDF处理流程"]
CheckPDF -- "否" --> Readability["Readability解析<br/>正文提取"]
Readability --> QualityCheck{"质量评估<br/>字数≥50且无错误？"}
QualityCheck -- "是" --> ReturnLocal["返回本地解析结果"]
QualityCheck -- "否" --> JinaFallback["Jina Reader回退<br/>处理SPA和反爬页面"]
JinaFallback --> Compare["内容质量比较"]
Compare --> BetterContent{"Jina内容更丰富？"}
BetterContent -- "是" --> ReturnJina["返回Jina解析结果"]
BetterContent -- "否" --> ReturnLocal
PDFFlow --> ReturnLocal
ReturnLocal --> End(["处理完成"])
ReturnJina --> End
```

**图表来源**
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L226-L269)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L171-L224)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L82-L126)

**章节来源**
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L1-L307)

### 智能内容分析器与自适应策略
**新增** 内容分析器实现了完整的智能内容分类与自适应分块策略，包括：

#### 内容特征分析
- **信息熵计算**：基于香农熵公式计算文本复杂度，衡量内容的信息密度
- **符号密度分析**：统计特殊符号和数字密度，识别技术文档特征
- **换行密度检测**：分析换行模式，识别列表、代码等结构化内容

#### 内容类型识别
- **表格检测**：支持 Markdown 表格和纯文本表格识别
- **代码块识别**：基于语法特征和缩进模式检测代码区域
- **列表判定**：识别有序和无序列表，支持多种标记格式

#### 密度分级系统
- **高密度内容**：技术文档、代码、数据密集型内容
- **中密度内容**：普通文本内容
- **低密度内容**：叙述性强、结构简单的文本

#### 自适应切分配置
- **高密度内容**：缩小块大小（400 tokens），增加重叠（15%）
- **低密度内容**：扩大块大小（1200 tokens），减少重叠（10%）
- **中密度内容**：使用默认配置（800 tokens，12.5%重叠）

```mermaid
flowchart TD
Start(["开始内容分析"]) --> Entropy["计算信息熵<br/>H = -Σ(p(x) * log2(p(x)))"]
Entropy --> Symbol["计算符号密度<br/>特殊符号+数字/总字符"]
Symbol --> Line["计算换行密度<br/>每100字符的换行次数"]
Line --> Type["检测内容类型<br/>表格/代码/列表/段落"]
Type --> Density["确定密度等级<br/>高/中/低"]
Density --> Config["生成自适应配置<br/>块大小+重叠调整"]
Config --> Reason["记录调整原因<br/>密度+特征描述"]
Reason --> End(["输出分析结果"])
```

**图表来源**
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L49-L68)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L74-L82)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L88-L94)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L99-L118)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L203-L241)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L246-L283)

**章节来源**
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L1-L476)

### 文本切分算法与自适应策略
- 切分优先级：Markdown 标题层级 > 段落 > 换行 > 中文句号/感叹号/问号 > 英文句号/感叹号/问号 > 空格 > 字符
- Token 估算：中文约 1.5 字符/token，其他约 4 字符/token，混合加权
- **自适应切分**：**更新** 基于内容分析器的智能配置，根据内容密度动态调整块大小与重叠
- 重叠窗口：以目标 token 数的倍数估算字符数，确保跨边界信息不丢失
- 输出元数据：包含页码、字符范围、token 数、来源类型、策略标识、自适应原因、内容分析结果

```mermaid
flowchart TD
Start(["开始切分"]) --> Analyze["内容分析<br/>信息熵/符号密度/换行密度/类型"]
Analyze --> Config["自适应配置<br/>块大小/重叠/原因"]
Config --> RecSplit["递归切分<br/>按优先级分隔符"]
RecSplit --> TokenEst["估算 token 数"]
TokenEst --> Enough{"小于等于目标大小？"}
Enough -- "是" --> Append["追加到当前块"]
Enough -- "否" --> Flush["保存当前块并计算重叠"]
Append --> RecSplit
Flush --> RecSplit
RecSplit --> Done(["结束"])
```

**图表来源**
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L109-L189)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L196-L283)

**章节来源**
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L21-L38)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L45-L56)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L109-L189)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L195-L260)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L265-L305)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L28-L34)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L196-L283)

### PDF 文本提取与扫描件检测
- 下载：从 Supabase Storage 按存储路径下载二进制
- 解析：动态导入 pdf-parse，异步解析文本、页数与元信息
- 页码映射：按平均字符数近似分配页起始字符位置
- 错误处理：区分加密、损坏、其他异常，返回结构化错误信息
- 扫描件检测：按平均每页字符数阈值判断是否为图片扫描版

```mermaid
sequenceDiagram
participant Proc as "处理器"
participant Store as "Supabase Storage"
participant Parser as "pdf-parse"
Proc->>Store : "下载 PDF 二进制"
Store-->>Proc : "Buffer"
Proc->>Parser : "解析 PDF"
Parser-->>Proc : "文本/页数/统计"
Proc->>Proc : "构建页码映射"
Proc->>Proc : "扫描件检测"
Proc-->>Proc : "返回结果或错误"
```

**图表来源**
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L41-L52)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L57-L117)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L133-L149)

**章节来源**
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L1-L150)
- [types/pdf-parse.d.ts](file://types/pdf-parse.d.ts#L1-L14)

### 网页内容清理与正文提取
**重构** 网页解析器现在采用三层处理策略，显著提升了内容提取的准确性和稳定性：

#### URL 类型检测
- **PDF链接**：直接标记为PDF类型，走PDF处理流程
- **YouTube链接**：标记为视频类型，直接保存链接
- **普通网页**：进入三层处理策略

#### 本地抓取与解析
- **超时控制**：30秒全局超时，读取内容有独立超时保护
- **User-Agent伪装**：使用真实浏览器User-Agent，模拟正常用户行为
- **错误处理**：针对401、403、404等HTTP状态码提供明确错误信息
- **Content-Type检查**：检测PDF内容类型，自动转换处理流程

#### Readability正文提取
- **DOM解析**：使用JSDOM构造文档对象，支持JavaScript渲染页面
- **正文提取**：使用@mozilla/readability提取主要内容
- **内容清理**：合并空白、规范化换行，计算词数
- **质量评估**：检查提取结果的有效性

#### Jina Reader回退机制
- **SPAs处理**：能够处理单页应用和反爬虫页面
- **Markdown输出**：返回结构化的Markdown内容
- **标题提取**：从Markdown第一行提取标题
- **内容清理**：移除Jina Reader的广告尾注

```mermaid
sequenceDiagram
participant Proc as "处理器"
participant Fetch as "fetch(带超时)"
participant Read as "Readability"
participant Jina as "Jina Reader"
Proc->>Fetch : "本地抓取 HTML"
Fetch-->>Proc : "HTML/Content-Type"
Proc->>Read : "Readability解析正文"
Read-->>Proc : "标题/内容/摘要"
Proc->>Proc : "质量评估与回退"
alt "质量不足或解析失败"
Proc->>Jina : "Jina Reader回退"
Jina-->>Proc : "Markdown内容"
Proc->>Proc : "内容比较与选择"
end
Proc->>Proc : "清理与统计"
```

**图表来源**
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L82-L126)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L131-L169)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L175-L224)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L226-L269)

**章节来源**
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L1-L307)

### 文本预处理：去重、标准化与噪声过滤
- 去重：基于内容哈希（content hash）在向量库层面进行去重，避免重复写入
- 标准化：切分前的 token 估算与字符范围记录，保证元数据一致性
- 噪声过滤：网页正文提取与清理步骤，去除广告、导航等非正文内容

**章节来源**
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L61-L63)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L265-L305)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L305-L310)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L146-L150)

### 分块策略与重叠处理
- 目标块大小与重叠：默认 800 tokens，12.5% 重叠；高密度内容缩小块并提高重叠，低密度内容扩大块并降低重叠
- 重叠计算：以目标 token 数的倍数估算字符数，保留尾部重叠部分参与下一块
- 上下文完整性：重叠窗口确保跨边界语义不被截断

**章节来源**
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L21-L38)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L252-L260)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L246-L283)

### 文本质量评估指标
- 词数/字符数：用于统计与进度展示
- 平均 token 数：切分后各块 token 数的均值
- 向量维度一致性：启动时强制校验，确保与数据库向量维度一致
- 日志与可观测性：向量操作成功/失败、耗时、插入/跳过数量、相似度均值

**章节来源**
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L66-L84)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L24-L30)
- [lib/processing/processor.ts](file://lib/processing/processor.ts#L138-L148)
- [lib/processing/processor.ts](file://lib/processing/processor.ts#L319-L329)
- [lib/config.ts](file://lib/config.ts#L7-L29)
- [lib/utils/logger.ts](file://lib/utils/logger.ts#L75-L94)

### 处理速度优化与内存控制
- 批量处理：嵌入生成按批处理，限制每批最大条数与单条最大 token 数
- 指数退避重试：对特定错误码进行退避重试，提升稳定性
- 分批写库：向量存储按固定批大小批量插入，减少往返开销
- 异步并行：API 层与业务层多处使用 Promise 并行，缩短端到端时间

**章节来源**
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L23-L26)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L115-L134)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L9-L10)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L108-L140)
- [app/api/sources/upload/route.ts](file://app/api/sources/upload/route.ts#L22-L26)
- [app/api/sources/text/route.ts](file://app/api/sources/text/route.ts#L44-L50)

### 自定义切分规则与特殊格式支持
- 自定义分隔符优先级：可在配置中扩展或调整
- **特殊格式识别**：**更新** 表格、代码块、列表的识别逻辑，影响密度与类型判断
- **回退机制**：**更新** 当 PDF 为扫描件或网页解析失败时，返回结构化错误信息，避免崩溃
- **三层处理策略**：**新增** 网页内容提取的智能回退机制，提升处理成功率

**章节来源**
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L24-L37)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L99-L191)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L122-L128)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L137-L168)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L226-L269)

### 处理失败的回退机制
- PDF：加密、损坏、其他异常分别返回明确错误
- **网页**：**重构** 实现三层处理策略，本地抓取失败时自动回退到Jina Reader
- 嵌入：API 错误、维度不匹配、重试上限触发时抛错并记录日志
- 存储：批量写入异常记录日志并上抛

**章节来源**
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L88-L116)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L93-L126)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L264-L269)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L90-L110)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L157-L172)

## 依赖关系分析
- 处理器依赖切分器、内容分析器、PDF/网页解析器、嵌入生成与向量存储
- **切分器依赖内容分析器**：**新增** 进行自适应配置
- 嵌入生成依赖配置中的模型与维度，并与向量存储交互
- API 路由负责创建 Source、入队与状态反馈
- **网页解析器新增依赖**：@mozilla/readability、jsdom

```mermaid
graph LR
APIU["upload/route.ts"] --> Proc["processor.ts"]
APIV["url/route.ts"] --> Proc
APIT["text/route.ts"] --> Proc
Proc --> PDFP["pdf-parser.ts"]
Proc --> WEBP["web-parser.ts"]
Proc --> Split["text-splitter.ts"]
Split --> Analyzer["content-analyzer.ts"]
Proc --> Emb["embedding.ts"]
Emb --> VS["vector-store.ts"]
WEBP --> Readability["@mozilla/readability"]
WEBP --> JSDOM["jsdom"]
Emb --> Cfg["config.ts"]
VS --> Log["logger.ts"]
```

**图表来源**
- [lib/processing/processor.ts](file://lib/processing/processor.ts#L1-L560)
- [lib/processing/text-splitter.ts](file://lib/processing/text-splitter.ts#L1-L432)
- [lib/processing/content-analyzer.ts](file://lib/processing/content-analyzer.ts#L1-L476)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L1-L150)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L1-L307)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L1-L189)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L1-L446)
- [lib/config.ts](file://lib/config.ts#L1-L187)
- [lib/utils/logger.ts](file://lib/utils/logger.ts#L1-L98)
- [app/api/sources/upload/route.ts](file://app/api/sources/upload/route.ts#L1-L111)
- [app/api/sources/url/route.ts](file://app/api/sources/url/route.ts#L1-L167)
- [app/api/sources/text/route.ts](file://app/api/sources/text/route.ts#L1-L122)
- [package.json](file://package.json#L21-L51)

**章节来源**
- [lib/processing/index.ts](file://lib/processing/index.ts#L1-L51)

## 性能考量
- 切分性能：优先使用语义边界，避免过细切分；自适应策略在高密度内容下减小块大小，提升检索精度
- 向量化性能：批量大小与单条 token 上限控制 API 调用成本；指数退避降低服务端压力
- 存储性能：批量插入与冲突忽略（ON CONFLICT DO NOTHING）减少写放大；相似度检索使用向量距离函数与阈值过滤
- 内存控制：切分与嵌入均采用流式/分批处理，避免一次性加载大文本
- **网页处理性能**：**新增** 三层处理策略在保证质量的同时控制了处理时间，Jina Reader回退机制避免了长时间等待
- 可观测性：统一日志结构，记录操作、耗时、成功/失败与关键指标，便于定位瓶颈

## 故障排查指南
- 向量维度不匹配：检查 EMBEDDING_DIM 配置与数据库向量维度一致
- PDF 解析失败：确认文件未加密、未损坏；若为扫描件，提示暂不支持 OCR
- **网页抓取失败**：**重构** 检查网络连通性、超时设置、HTTP状态码；查看三层处理策略的错误日志
- **Jina Reader配置**：**新增** 需要JINA_API_KEY环境变量，否则会使用免费版本但可能有访问限制
- 嵌入 API 失败：核对鉴权信息、模型名与维度；查看重试日志
- 写库失败：检查数据库连接、权限与批量 SQL 构造

**章节来源**
- [lib/config.ts](file://lib/config.ts#L17-L29)
- [lib/processing/pdf-parser.ts](file://lib/processing/pdf-parser.ts#L88-L116)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L93-L126)
- [lib/processing/web-parser.ts](file://lib/processing/web-parser.ts#L183-L186)
- [lib/processing/embedding.ts](file://lib/processing/embedding.ts#L90-L110)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L157-L172)

## 结论
该系统以"递归语义切分 + 内容自适应 + 多格式提取 + 批量向量化 + 向量存储"为核心，形成从知识源到检索增强的完整链路。**重构的Web解析器模块**通过三层处理策略（本地抓取+Readability解析+Jina Reader回退机制），显著提升了网页内容提取的稳定性和准确性。**新增的内容分析器模块**通过智能内容分类与自适应分块策略，进一步优化了不同内容类型的处理效果。通过严格的维度校验、指数退避重试、批量写入与可观测日志，兼顾了正确性、性能与可维护性。建议在生产环境中结合实际数据分布进一步调优切分参数与批大小，并完善扫描件 OCR 与视频内容提取能力。

## 附录
- API 入口职责
  - 上传 PDF：校验类型与大小，上传至 Supabase Storage，创建 Source 并入队
  - 添加 URL：检测类型（PDF/YouTube/网页），必要时转为 PDF 流程或直接标记不支持
  - 添加文字：校验内容长度与去重，直接创建 Source 并入队
- 数据库与向量
  - 向量维度固定为 1024，使用 HNSW 向量索引与全文检索（TSV）组合
  - 批量插入与相似度检索均带有阈值与 TopK 控制
- **内容分析器测试**
  - **新增** 提供完整的自适应切分功能测试脚本，验证不同内容类型的处理效果
  - 支持代码、表格、叙述性文本、技术文档等样本的自动测试
- **新增依赖说明**
  - **@mozilla/readability**：用于从HTML中提取正文内容
  - **jsdom**：用于解析HTML文档对象，支持JavaScript渲染页面
  - **Jina Reader**：用于处理SPA和反爬虫页面，提供Markdown格式内容

**章节来源**
- [app/api/sources/upload/route.ts](file://app/api/sources/upload/route.ts#L14-L111)
- [app/api/sources/url/route.ts](file://app/api/sources/url/route.ts#L20-L167)
- [app/api/sources/text/route.ts](file://app/api/sources/text/route.ts#L13-L122)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L128-L140)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L215-L252)
- [lib/db/vector-store.ts](file://lib/db/vector-store.ts#L363-L429)
- [scripts/test-adaptive-splitting.ts](file://scripts/test-adaptive-splitting.ts#L1-L113)
- [package.json](file://package.json#L21-L51)